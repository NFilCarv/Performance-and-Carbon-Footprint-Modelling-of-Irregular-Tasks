{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42804086",
   "metadata": {},
   "source": [
    "### NOTEBOOK SUMMARY\n",
    "\n",
    "This notebook contains a summarized way to obtain all models used in the final model of full job characteristic predictions. Additionally, after training and saving each model there are two assembling funtions for the models for full predictions, the first one being notebook prone and the second one a sketch of what a it would look like in a cluster enviromnent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38673f",
   "metadata": {},
   "source": [
    "The Following code block contains the model used for the process of classification of jobs as deterministic (regular) or nondeterministic (irregular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Upload the downloaded dataset 'HPCsyntheticdata.parquet'.\n",
    "df_classification = pd.read_parquet('/your/path/HPCsyntheticdata.parquet', engine='pyarrow')\n",
    "\n",
    "# Features used in the training process of the classifier model between regular and irregular jobs.\n",
    "X_classification = df_classification[['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'qos', 'priority', 'eligible_time_epoch']].values\n",
    "y_label = df_classification['label'].values  # Assuming 'label' is the target column\n",
    "\n",
    "# Split data for training and testing (add a validation split optionally as well)\n",
    "X_train_classification, X_test_classification, y_train_classification, y_test_classification = train_test_split(\n",
    "    X_classification, y_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling data with a MaxAbsScaler for uniformity.\n",
    "scaler_classification = MaxAbsScaler()\n",
    "X_train_classification_scaled = scaler_classification.fit_transform(X_train_classification)\n",
    "X_test_classification_scaled = scaler_classification.transform(X_test_classification)\n",
    "\n",
    "# Initialize and train ExtraTreesClassifier (defined weights for more false negatives than false positives)\n",
    "class_weights = {0: 1, 1: 100}\n",
    "classifier_model = ExtraTreesClassifier(n_estimators=50, class_weight=class_weights, random_state=42, n_jobs=-1)\n",
    "classifier_model.fit(X_train_classification_scaled, y_train_classification)\n",
    "\n",
    "# Save the classification model and scaler to be uploaded to the final model.\n",
    "joblib.dump(classifier_model, 'classifier_model.joblib')\n",
    "joblib.dump(scaler_classification, 'scaler_classification.joblib')\n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c6797",
   "metadata": {},
   "source": [
    "After the classification process the next step lies on the characterization of the irregular jobs. In the following code block the model predicts the mean power consumption of the job during it's execution.\n",
    "\n",
    "The dataset is, in order to evaluate only irregular jobs, split by removing all jobs labeled as '1' (regular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification = pd.read_parquet('/your/path/HPCsyntheticdata.parquet', engine='pyarrow')\n",
    "# Filter the dataset to keep only the irregular jobs (labeled as 0).\n",
    "df_regression_filtered = df_classification[df_classification['label'] == 0].copy()\n",
    "\n",
    "df_regression_filtered = df_regression_filtered[['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch', 'mean_power','run_time']]\n",
    "\n",
    "# Save the filtered regression dataset\n",
    "df_regression_filtered.to_parquet('/your/path/df_completed_filtered_only_0s_meanpower.parquet', engine='pyarrow')\n",
    "\n",
    "# Load the new dataset.\n",
    "df_regression = pd.read_parquet('/your/path/df_completed_filtered_only_0s_meanpower.parquet', engine='pyarrow')\n",
    "\n",
    "# Define features and target for regression\n",
    "X_regression = df_regression[['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch']].values\n",
    "y_power = df_regression['mean_power'].values.reshape(-1, 1)  \n",
    "\n",
    "# Split data for regression (optionally add a validation split)\n",
    "X_train_regression, X_test_regression, y_train_power, y_test_power = train_test_split(\n",
    "    X_regression, y_power, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply MinMaxScaler for data uniformity\n",
    "scaler_regression = MinMaxScaler()\n",
    "X_train_regression_scaled = scaler_regression.fit_transform(X_train_regression)\n",
    "X_test_regression_scaled = scaler_regression.transform(X_test_regression)\n",
    "\n",
    "# Scale the target variable (mean_power)\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_power_scaled = y_scaler.fit_transform(y_train_power)\n",
    "y_test_power_scaled = y_scaler.transform(y_test_power)\n",
    "\n",
    "# Initialize and train RandomForestRegressor\n",
    "regression_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "regression_model.fit(X_train_regression_scaled, y_train_power_scaled.ravel())\n",
    "\n",
    "# Save the regression model and scalers\n",
    "joblib.dump(regression_model, 'regression_model.joblib')\n",
    "joblib.dump(scaler_regression, 'scaler_regression.joblib')\n",
    "joblib.dump(y_scaler, 'y_scaler.joblib')\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810649a7",
   "metadata": {},
   "source": [
    "In addition to predicting Power consumption of the job it is, below, created a model to predict the run time of that same job. In order to do so the jobs are split into the interval of run time they belong to with a multiclassifier model as the range of values in the dataset is very large. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e0f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (ExtraTreesClassifier, StackingClassifier, \n",
    "                              ExtraTreesRegressor, RandomForestRegressor)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from xgboost import XGBClassifier  \n",
    "\n",
    "# Load the regression dataset with error handling\n",
    "df = pd.read_parquet('your/path/df_completed_filtered_only_0s_meanpower.parquet', engine='pyarrow')\n",
    "\n",
    "X = df[['inst', 'bi', 'num_gpus_req', 'mem_req', \n",
    "         'num_cores_req', 'num_tasks', 'qos', \n",
    "         'priority', 'eligible_time_epoch']].values\n",
    "y = df['run_time'].values\n",
    "\n",
    "# Calculation of the key percentiles for multiclass classification\n",
    "percentile_25th = np.percentile(y, 25)\n",
    "percentile_50th = np.percentile(y, 50)\n",
    "percentile_75th = np.percentile(y, 75)\n",
    "mean_y = np.mean(y)\n",
    "percentile_95th = np.percentile(y, 95)\n",
    "percentile_99th = np.percentile(y, 99)  \n",
    "\n",
    "# Multiclass target based on percentiles\n",
    "def create_multiclass_target(value):\n",
    "    if value <= percentile_25th:\n",
    "        return 0\n",
    "    elif percentile_25th < value <= percentile_50th:\n",
    "        return 1\n",
    "    elif percentile_50th < value <= percentile_75th:\n",
    "        return 2\n",
    "    elif percentile_75th < value <= mean_y:\n",
    "        return 3\n",
    "    elif mean_y < value <= percentile_95th:\n",
    "        return 4\n",
    "    elif percentile_95th < value <= percentile_99th:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "y_multiclass = np.array([create_multiclass_target(val) for val in y])\n",
    "\n",
    "# Split the dataset in optional splits \n",
    "X_train, X_test, y_train_multiclass, y_test_multiclass = train_test_split(X, y_multiclass, test_size=0.1, random_state=42)\n",
    "\n",
    "# Scale the features using MinMaxScaler for uniformity\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Base learners used for the stacking model.\n",
    "base_learners = [\n",
    "    ('et', ExtraTreesClassifier(n_estimators=500, random_state=42)),\n",
    "    ('SGD', SGDClassifier(max_iter=3000, tol=1e-4, alpha=1e-5, \n",
    "                          early_stopping=True, learning_rate='adaptive', \n",
    "                          eta0=0.01, loss='modified_huber', \n",
    "                          class_weight='balanced', n_iter_no_change=10)),\n",
    "    ('xgb', XGBClassifier(n_estimators=500, learning_rate=0.05, \n",
    "                          subsample=0.8, random_state=42))\n",
    "]\n",
    "\n",
    "# Meta model (Logistic Regression)\n",
    "meta_model = LogisticRegression(C=1.0, max_iter=3000, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_model)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train_scaled, y_train_multiclass)\n",
    "\n",
    "# Predictions and results if you want to vizualise the accuracy for the test data.\n",
    "y_test_pred_multiclass = stacking_clf.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test_multiclass, y_test_pred_multiclass)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "\n",
    "# Save the stacking model and scaler\n",
    "joblib.dump(stacking_clf, 'multiclass_stacking_classifier_model_6classes.pkl')\n",
    "joblib.dump(scaler, 'minmax_scaler.pkl')\n",
    "print(\"Multiclass model and scaler saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b489905",
   "metadata": {},
   "source": [
    "Leveraging on the new smaller intervals each class (interval) is now assigned a scaler and the regressor model that best fits the data in each class in order to finalize the run time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297aa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (ExtraTreesClassifier, StackingClassifier, \n",
    "                              ExtraTreesRegressor, RandomForestRegressor)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from xgboost import XGBClassifier  \n",
    "\n",
    "df = pd.read_parquet('your/path/df_completed_filtered_only_0s_meanpower.parquet', engine='pyarrow')\n",
    "\n",
    "X = df[['inst', 'bi', 'num_gpus_req', 'mem_req', \n",
    "         'num_cores_req', 'num_tasks', 'qos', \n",
    "         'priority', 'eligible_time_epoch']].values\n",
    "y = df['run_time'].values\n",
    "\n",
    "\n",
    "# Load saved classifier and scaler\n",
    "stacking_clf = joblib.load('multiclass_stacking_classifier_model_6classes.pkl')\n",
    "scaler = joblib.load('minmax_scaler.pkl')\n",
    "\n",
    "# Predict class labels for regression\n",
    "X_scaled = scaler.transform(X)\n",
    "y_pred_classes = stacking_clf.predict(X_scaled)\n",
    "\n",
    "# Defining the subsets of best performing features per class \n",
    "\n",
    "feature_subsets = {\n",
    "    0: ['num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'], \n",
    "    1: ['inst', 'num_gpus_req', 'num_cores_req', 'qos', 'eligible_time_epoch'], \n",
    "    2: ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'priority', 'eligible_time_epoch'], \n",
    "    3: ['inst', 'bi', 'num_gpus_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'], \n",
    "    4: ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "    5: ['num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "    6: ['bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch']\n",
    "}\n",
    "\n",
    "\n",
    "# Initializing dictionaries for scalers and regressors considering the different classes of regressions.\n",
    "scalers = {}\n",
    "regressors = {}\n",
    "train_y_true, train_y_pred, test_y_true, test_y_pred, val_y_true, val_y_pred = [], [], [], [], [], []\n",
    "\n",
    "# Training regressors per class\n",
    "for class_label in range(7):\n",
    "    class_indices = np.where(y_pred_classes == class_label)[0]\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No instances predicted for class {class_label}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    selected_features = feature_subsets.get(class_label, None)\n",
    "    if not selected_features:\n",
    "        print(f\"No feature subset defined for class {class_label}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    X_class = df[selected_features].values[class_indices]\n",
    "    y_class = y[class_indices]\n",
    "\n",
    "    # Split into train, validation, and test sets \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
    "    X_val, y_val = X_temp[:int(len(X_temp) * 0.5)], y_temp[:int(len(X_temp) * 0.5)]\n",
    "    X_test, y_test = X_temp[int(len(X_temp) * 0.5):], y_temp[int(len(X_temp) * 0.5):]\n",
    "\n",
    "    # Definning and fitting the correct scaler for the each class\n",
    "    if class_label in [0, 1, 2, 3]:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    elif class_label == 5:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        scaler = None \n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = X_train, X_val, X_test\n",
    "\n",
    "    # Save the scalers for future use\n",
    "    scalers[class_label] = scaler\n",
    "\n",
    "    # Assigning the appropriate regressor for the each class\n",
    "    if class_label in [0, 1]:\n",
    "        regressor = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    elif class_label in [3, 4]:\n",
    "        regressor = ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    elif class_label in [2, 5]:\n",
    "        regressor = ExtraTreesRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "    else:\n",
    "        regressor = HuberRegressor()\n",
    "\n",
    "    # Training the regressor for the current class\n",
    "    regressor.fit(X_train_scaled, y_train)\n",
    "    regressors[class_label] = regressor  # Save the trained regressor\n",
    "\n",
    "    # Store the training, validation, and test predictions and actual values if you want to visualize results.\n",
    "    train_y_true.extend(y_train)\n",
    "    train_y_pred.extend(regressor.predict(X_train_scaled).astype('int32'))\n",
    "    test_y_true.extend(y_test)\n",
    "    test_y_pred.extend(regressor.predict(X_test_scaled).astype('int32'))\n",
    "    val_y_true.extend(y_val)\n",
    "    val_y_pred.extend(regressor.predict(X_val_scaled).astype('int32'))\n",
    "\n",
    "# Calculate the overall mean absolute error if you want to visualize results\n",
    "overall_mae = mean_absolute_error(np.concatenate((train_y_true, test_y_true, val_y_true)), \n",
    "                                   np.concatenate((train_y_pred, test_y_pred, val_y_pred)))\n",
    "print(f\"Overall Mean Absolute Error: {overall_mae:.2f}\")\n",
    "\n",
    "# Save each trained regressor model and the corresponding scaler for each class\n",
    "for class_label in range(7):\n",
    "    if class_label not in regressors:\n",
    "        continue\n",
    "    joblib.dump(regressors[class_label], f'regressor_model_class_{class_label}.pkl')\n",
    "    if scalers[class_label]:\n",
    "        joblib.dump(scalers[class_label], f'scaler_class_{class_label}.pkl')\n",
    "    print(f'Regressor and scaler (if applicable) for class {class_label} saved successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be81c2",
   "metadata": {},
   "source": [
    "Let us now obtain a sample of each type of job in order to run in the fully built model to exemplify its behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd39f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random value with label 0:\n",
      "inst                     798643.0\n",
      "bi                         3091.0\n",
      "mem_req                       8.0\n",
      "num_gpus_req                    1\n",
      "num_cores_req                  16\n",
      "num_tasks                    16.0\n",
      "qos                             1\n",
      "priority                   298700\n",
      "eligible_time_epoch    1595485046\n",
      "total_power                  2530\n",
      "mean_power                    843\n",
      "run_time                     53.0\n",
      "label                         0.0\n",
      "Name: 56580, dtype: object\n",
      "\n",
      "Random value with label 1:\n",
      "inst                   245516.790698\n",
      "bi                        864.075323\n",
      "mem_req                    237.75576\n",
      "num_gpus_req                       4\n",
      "num_cores_req                    128\n",
      "num_tasks                        4.0\n",
      "qos                                1\n",
      "priority                      158434\n",
      "eligible_time_epoch       1601983998\n",
      "total_power                      860\n",
      "mean_power                       860\n",
      "run_time                    2.016206\n",
      "label                            1.0\n",
      "Name: 275144, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "df1 = pd.read_parquet('HPCsyntheticdata.parquet', engine='pyarrow')\n",
    "\n",
    "# Filter the dataframe by label\n",
    "df_label_0 = df1[df1['label'] == 0]\n",
    "df_label_1 = df1[df1['label'] == 1]\n",
    "\n",
    "# Select a random sample from each label group\n",
    "random_value_label_0 = df_label_0.sample(n=1).iloc[0]\n",
    "random_value_label_1 = df_label_1.sample(n=1).iloc[0]\n",
    "\n",
    "# Print the random values\n",
    "print(\"Random value with label 0:\")\n",
    "print(random_value_label_0)\n",
    "\n",
    "print(\"\\nRandom value with label 1:\")\n",
    "print(random_value_label_1)\n",
    "\n",
    "# These features from 2 random jobs are used below as examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff3090",
   "metadata": {},
   "source": [
    "The following code block contains a aggreagation of all models to exemplify the structure of the full model would have in the HPC cluster. This version is executable in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392d6036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result: {'classification': 0, 'predicted_class': 1, 'predicted_runtime': 49.56, 'predicted_mean_power': 847.5100000000002}\n",
      "Time taken for prediction: 0.6258 seconds\n",
      "Prediction result: {'classification': 1, 'predicted_class': None, 'predicted_runtime': None, 'predicted_mean_power': None}\n",
      "Time taken for prediction: 0.0420 seconds\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # Import the time module for measuring execution time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load all models and scalers once at the start\n",
    "classifier_model = joblib.load('classifier_model.joblib')\n",
    "scaler_classification = joblib.load('scaler_classification.joblib')\n",
    "\n",
    "# Load regression scalers for runtime and mean power\n",
    "scalers_regression = {}\n",
    "for class_label in range(7):\n",
    "    scaler_path = f'scaler_class_{class_label}.pkl'\n",
    "    if os.path.exists(scaler_path):\n",
    "        scalers_regression[class_label] = joblib.load(scaler_path)\n",
    "\n",
    "# Load regression models for runtime\n",
    "regressors_runtime = {class_label: joblib.load(f'regressor_model_class_{class_label}.pkl') for class_label in range(7)}\n",
    "\n",
    "# Load mean power regression model and scaler\n",
    "mean_power_model = joblib.load('regression_model.joblib')\n",
    "mean_power_scaler = joblib.load('scaler_regression.joblib')\n",
    "y_scaler = joblib.load('y_scaler.joblib')\n",
    "\n",
    "# Load multiclass classifier and scaler once\n",
    "stacking_clf = joblib.load('multiclass_stacking_classifier_model_6classes.pkl')\n",
    "scaler_multiclass = joblib.load('minmax_scaler.pkl')\n",
    "\n",
    "# Function to scale input for classification without 'num_tasks'\n",
    "def scale_for_classification(input_data):\n",
    "    features_to_scale = ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'qos', 'priority', 'eligible_time_epoch']\n",
    "    input_array = np.array([[input_data[feature] for feature in features_to_scale]])\n",
    "    X_scaled = scaler_classification.transform(input_array)\n",
    "    return X_scaled\n",
    "\n",
    "# Prediction workflow function\n",
    "def predict_workflow(inputs):\n",
    "    try:\n",
    "        # Step 1: Scale input data for binary classification (excluding 'num_tasks')\n",
    "        X_scaled_classification = scale_for_classification(inputs)\n",
    "\n",
    "        # Step 2: Binary classification\n",
    "        classification = classifier_model.predict(X_scaled_classification)[0]\n",
    "\n",
    "        # If classification predicts 1, return and stop the process\n",
    "        if classification == 1:\n",
    "            return {\n",
    "                'classification': 1,\n",
    "                'predicted_class': None,\n",
    "                'predicted_runtime': None,\n",
    "                'predicted_mean_power': None\n",
    "            }\n",
    "\n",
    "        # Step 3: Predict mean power\n",
    "        input_array = np.array([[inputs['inst'], inputs['bi'], inputs['num_gpus_req'], inputs['mem_req'],\n",
    "                                 inputs['num_cores_req'], inputs['num_tasks'], inputs['qos'],\n",
    "                                 inputs['priority'], inputs['eligible_time_epoch']]])\n",
    "        \n",
    "        X_scaled_power = mean_power_scaler.transform(input_array)\n",
    "        predicted_mean_power_scaled = mean_power_model.predict(X_scaled_power)\n",
    "        predicted_mean_power = y_scaler.inverse_transform(predicted_mean_power_scaled.reshape(-1, 1))[0, 0]\n",
    "\n",
    "        # Step 4: Multiclass classification\n",
    "        X_scaled_multiclass = scaler_multiclass.transform(input_array)\n",
    "        predicted_class = stacking_clf.predict(X_scaled_multiclass)[0]\n",
    "\n",
    "        # Step 5: Select feature subset for regression\n",
    "        feature_subsets = {\n",
    "            0: ['num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            1: ['inst', 'num_gpus_req', 'num_cores_req', 'qos', 'eligible_time_epoch'],\n",
    "            2: ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'priority', 'eligible_time_epoch'],\n",
    "            3: ['inst', 'bi', 'num_gpus_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            4: ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            5: ['num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            6: ['bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch']\n",
    "        }\n",
    "\n",
    "        selected_features = feature_subsets[predicted_class]\n",
    "        X_class = np.array([[inputs[feature] for feature in selected_features]])\n",
    "\n",
    "        # Step 6: Scale features for regression\n",
    "        scaler_runtime = scalers_regression.get(predicted_class, None)\n",
    "        X_scaled_runtime = scaler_runtime.transform(X_class) if scaler_runtime else X_class\n",
    "\n",
    "        # Step 7: Predict runtime\n",
    "        regressor_runtime = regressors_runtime[predicted_class]\n",
    "        predicted_runtime = regressor_runtime.predict(X_scaled_runtime)[0]\n",
    "\n",
    "        return {\n",
    "            'classification': 0,\n",
    "            'predicted_class': predicted_class,\n",
    "            'predicted_runtime': predicted_runtime,\n",
    "            'predicted_mean_power': predicted_mean_power\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "# Example prediction for a single input\n",
    "example_input_0 = {\n",
    "    'inst': 798643,\n",
    "    'bi': 3091,\n",
    "    'num_gpus_req': 1,\n",
    "    'mem_req': 8,\n",
    "    'num_cores_req': 16,\n",
    "    'num_tasks': 16,\n",
    "    'qos': 1,\n",
    "    'priority': 298700,\n",
    "    'eligible_time_epoch': 1595485046\n",
    "}\n",
    "\n",
    "\n",
    "example_input_1 = {\n",
    "    'inst': 245517,\n",
    "    'bi': 864,\n",
    "    'num_gpus_req': 4,\n",
    "    'mem_req': 238,\n",
    "    'num_cores_req': 128,\n",
    "    'num_tasks': 4,\n",
    "    'qos': 1,\n",
    "    'priority': 158434,\n",
    "    'eligible_time_epoch': 1601983998\n",
    "}\n",
    "\n",
    "\n",
    "# Measure time for prediction\n",
    "start_time_0 = time.time()\n",
    "\n",
    "# Single prediction for example inputs\n",
    "result_0 = predict_workflow(example_input_0)\n",
    "\n",
    "end_time_0 = time.time()\n",
    "execution_time_0 = end_time_0 - start_time_0\n",
    "\n",
    "# Print the prediction result and the time taken\n",
    "print(\"Prediction result:\", result_0)\n",
    "print(f\"Time taken for prediction: {execution_time_0:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Measure time for prediction\n",
    "start_time_1 = time.time()\n",
    "\n",
    "# Single prediction for example inputs\n",
    "result_1 = predict_workflow(example_input_1)\n",
    "\n",
    "end_time_1 = time.time()\n",
    "execution_time_1 = end_time_1 - start_time_1\n",
    "\n",
    "# Print the prediction result and the time taken\n",
    "print(\"Prediction result:\", result_1)\n",
    "print(f\"Time taken for prediction: {execution_time_1:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33733c",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "The code block below is a sketch of how the function would look like in an HPC cluster environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31347de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys  # For command-line argument parsing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import argparse  # For argument parsing\n",
    "\n",
    "#_______________________________________________________________________________________________________________#\n",
    "# Load all models and scalers once at the start (This would be done separately as it is necessary to do only once)\n",
    "classifier_model = joblib.load('classifier_model.joblib')\n",
    "scaler_classification = joblib.load('scaler_classification.joblib')\n",
    "\n",
    "# Load regression scalers for runtime and mean power\n",
    "scalers_regression = {}\n",
    "for class_label in range(7):\n",
    "    scaler_path = f'scaler_class_{class_label}.pkl'\n",
    "    if os.path.exists(scaler_path):\n",
    "        scalers_regression[class_label] = joblib.load(scaler_path)\n",
    "\n",
    "# Load regression models for runtime\n",
    "regressors_runtime = {class_label: joblib.load(f'regressor_model_class_{class_label}.pkl') for class_label in range(7)}\n",
    "\n",
    "# Load mean power regression model and scaler\n",
    "mean_power_model = joblib.load('regression_model.joblib')\n",
    "mean_power_scaler = joblib.load('scaler_regression.joblib')\n",
    "y_scaler = joblib.load('y_scaler.joblib')\n",
    "\n",
    "# Load multiclass classifier and scaler once\n",
    "stacking_clf = joblib.load('multiclass_stacking_classifier_model_6classes.pkl')\n",
    "scaler_multiclass = joblib.load('minmax_scaler.pkl')\n",
    "\n",
    "#_________________________________________________________________________________________________________________#\n",
    "\n",
    "\n",
    "# Function to scale input for classification without 'num_tasks'\n",
    "def scale_for_classification(input_data):\n",
    "    features_to_scale = ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'qos', 'priority', 'eligible_time_epoch']\n",
    "    input_array = np.array([[input_data[feature] for feature in features_to_scale]])\n",
    "    X_scaled = scaler_classification.transform(input_array)\n",
    "    return X_scaled\n",
    "\n",
    "# Prediction workflow function\n",
    "def predict_workflow(inputs):\n",
    "    try:\n",
    "        # Scaling input data for binary classification (excluding 'num_tasks')\n",
    "        X_scaled_classification = scale_for_classification(inputs)\n",
    "\n",
    "        # Binary classification\n",
    "        classification = classifier_model.predict(X_scaled_classification)[0]\n",
    "\n",
    "        # If classification predicts 1, return and stop the process\n",
    "        if classification == 1:\n",
    "            return {\n",
    "                'classification': 1,\n",
    "                'predicted_class': None,\n",
    "                'predicted_runtime': None,\n",
    "                'predicted_mean_power': None\n",
    "            }\n",
    "\n",
    "        # Predict mean power\n",
    "        input_array = np.array([[inputs['inst'], inputs['bi'], inputs['num_gpus_req'], inputs['mem_req'],\n",
    "                                 inputs['num_cores_req'], inputs['num_tasks'], inputs['qos'],\n",
    "                                 inputs['priority'], inputs['eligible_time_epoch']]])\n",
    "        \n",
    "        X_scaled_power = mean_power_scaler.transform(input_array)\n",
    "        predicted_mean_power_scaled = mean_power_model.predict(X_scaled_power)\n",
    "        predicted_mean_power = y_scaler.inverse_transform(predicted_mean_power_scaled.reshape(-1, 1))[0, 0]\n",
    "\n",
    "        # Multiclass classification for run time prediction\n",
    "        X_scaled_multiclass = scaler_multiclass.transform(input_array)\n",
    "        predicted_class = stacking_clf.predict(X_scaled_multiclass)[0]\n",
    "\n",
    "        # Select feature subset for regression in run time prediction\n",
    "        feature_subsets = {\n",
    "            0: ['num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            1: ['inst', 'num_gpus_req', 'num_cores_req', 'qos', 'eligible_time_epoch'],\n",
    "            2: ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'priority', 'eligible_time_epoch'],\n",
    "            3: ['inst', 'bi', 'num_gpus_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            4: ['inst', 'bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            5: ['num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch'],\n",
    "            6: ['bi', 'num_gpus_req', 'mem_req', 'num_cores_req', 'num_tasks', 'qos', 'priority', 'eligible_time_epoch']\n",
    "        }\n",
    "\n",
    "        selected_features = feature_subsets[predicted_class]\n",
    "        X_class = np.array([[inputs[feature] for feature in selected_features]])\n",
    "\n",
    "        # Scale features\n",
    "        scaler_runtime = scalers_regression.get(predicted_class, None)\n",
    "        X_scaled_runtime = scaler_runtime.transform(X_class) if scaler_runtime else X_class\n",
    "\n",
    "        # Predict runtime\n",
    "        regressor_runtime = regressors_runtime[predicted_class]\n",
    "        predicted_runtime = regressor_runtime.predict(X_scaled_runtime)[0]\n",
    "\n",
    "        return {\n",
    "            'classification': 0,\n",
    "            'predicted_class': predicted_class,\n",
    "            'predicted_runtime': predicted_runtime,\n",
    "            'predicted_mean_power': predicted_mean_power\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Command-line argument parsing function\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run prediction before scheduling a job in the HPC cluster.\")\n",
    "    parser.add_argument('--inst', type=int, required=True, help=\"Instruction count\")\n",
    "    parser.add_argument('--bi', type=int, required=True, help=\"Branch instruction count\")\n",
    "    parser.add_argument('--num_gpus_req', type=int, required=True, help=\"Number of GPUs required\")\n",
    "    parser.add_argument('--mem_req', type=int, required=True, help=\"Memory required (in GB)\")\n",
    "    parser.add_argument('--num_cores_req', type=int, required=True, help=\"Number of cores required\")\n",
    "    parser.add_argument('--num_tasks', type=int, required=True, help=\"Number of tasks\")\n",
    "    parser.add_argument('--qos', type=int, required=True, help=\"QoS level\")\n",
    "    parser.add_argument('--priority', type=int, required=True, help=\"Job priority\")\n",
    "    parser.add_argument('--eligible_time_epoch', type=int, required=True, help=\"Eligible time in epoch format\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "# Main function to run prediction\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    input_data = {\n",
    "        'inst': args.inst,\n",
    "        'bi': args.bi,\n",
    "        'num_gpus_req': args.num_gpus_req,\n",
    "        'mem_req': args.mem_req,\n",
    "        'num_cores_req': args.num_cores_req,\n",
    "        'num_tasks': args.num_tasks,\n",
    "        'qos': args.qos,\n",
    "        'priority': args.priority,\n",
    "        'eligible_time_epoch': args.eligible_time_epoch\n",
    "    }\n",
    "\n",
    "    # Measure time for prediction\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run prediction\n",
    "    result = predict_workflow(input_data)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Output the prediction result and the time taken\n",
    "    print(\"Prediction result:\", result)\n",
    "    print(f\"Time taken for prediction: {execution_time:.4f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
